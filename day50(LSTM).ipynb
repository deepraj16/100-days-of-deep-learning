{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNiMxRaFnJivABIOx/SKBIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepraj16/100-days-of-deep-learning/blob/main/day50(LSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eqwk2PzwcZw"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=\"\"\" Machine Learning Interview Questions For Freshers\n",
        "1. What are some real-life applications of clustering algorithms?\n",
        "Clustering algorithms are used in various real-life applications such as:\n",
        "Customer segmentation for targeted marketing\n",
        "Recommendation systems for personalized suggestions\n",
        "Anomaly detection in fraud prevention\n",
        "Image compression to reduce storage\n",
        "Healthcare for grouping patients with similar conditions\n",
        "Document categorization in search engines\n",
        "2. How to choose an optimal number of clusters?\n",
        "Elbow Method: Plot the explained variance or within-cluster sum of squares (WCSS) against the number of clusters. The “elbow” point, where the curve starts to flatten, indicates the optimal number of clusters.\n",
        "Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters. The optimal number of clusters is the one with the highest average silhouette score.\n",
        "Gap Statistic: Compares the clustering result with a random clustering of the same data. A larger gap between the real and random clustering suggests a more appropriate number of clusters.\n",
        "3. What is feature engineering? How does it affect the model’s performance?\n",
        "Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations.\n",
        "Also, there are times when multiple pieces of information are clubbed and provided as a single data column. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot.\n",
        "4. What is overfitting in machine learning and how can it be avoided?\n",
        "Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier.\n",
        "To avoid overfitting there are multiple methods that we can use:\n",
        "Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.\n",
        "Using regularization methods like L1 or L2 regularization which is used to penalize the model’s weights to avoid overfitting.\n",
        "5. Why we cannot use linear regression for a classification task?\n",
        "The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values.\n",
        "If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task.\n",
        "6. Why do we perform normalization?\n",
        "To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth.\n",
        "7. What is the difference between precision and recall?\n",
        "Precision is the ratio between the true positives(TP) and all the positive examples (TP+FP) predicted by the model. In other words, precision measures how many of the predicted positive examples are actually true positives. It is a measure of the model’s ability to avoid false positives and make accurate positive predictions.\n",
        "Precision=TPTP+FPPrecision=TP+FPTP\n",
        "In recall, we calculate the ratio of true positives (TP) and the total number of examples (TP+FN) that actually fall in the positive class. Recall measures how many of the actual positive examples are correctly identified by the model. It is a measure of the model’s ability to avoid false negatives and identify all positive examples correctly.\n",
        "Recall=TPTP+FNRecall=TP+FNTP\n",
        "8. What is the difference between upsampling and downsampling?\n",
        "In upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class and adding them to the dataset repeat this process till the dataset gets balanced for each class. But, here is a disadvantage the training accuracy becomes high as in each epoch model trained more than once in each epoch but the same high accuracy is not observed in the validation accuracy.\n",
        "In downsampling, we decrease the number of samples in the majority class by selecting some random number of points that are equal to the number of data points in the minority class so that the distribution becomes balanced. In this case, we have to suffer from data loss which may lead to the loss of some critical information as well.\n",
        "9. What is data leakage and how can we identify it?\n",
        "If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable’s information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage.\n",
        "Macro Average: The average of precision, recall, and F1-score across all classes, treating them equally.\n",
        "Weighted Average: The average of metrics, weighted by class support, giving more importance to frequent classes.\n",
        "20. What is the difference between the k-means and k-means++ algorithms?\n",
        "The only difference between the two is in the way centroids are initialized. In the k-means algorithm, the centroids are initialized randomly from the given points. There is a drawback in this method that sometimes this random initialization leads to non-optimized clusters due to maybe initialization of two clusters close to each other.\n",
        "To overcome this problem k-means++ algorithm was formed. In k-means++, the first centroid is selected randomly from the data points. The selection of subsequent centroids is based on their separation from the initial centroids. The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected. This guarantees that the centroids are evenly spread apart and lowers the possibility of convergence to less-than-ideal clusters. This helps the algorithm reach the global minima instead of getting stuck at some local minima.\n",
        "Read more about it here.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aLl9a2_zwgMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([k])"
      ],
      "metadata": {
        "id": "eCCyy2r0wsj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "9M8YqkPBwu0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in k.split('\\n') :\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "XMKkrOUXwxNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in k.split('\\n') :\n",
        "    print(tokenizer.texts_to_sequences([sentence]))"
      ],
      "metadata": {
        "id": "qHu4Qy9Ewz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sq=[]\n",
        "for sentence in k.split('\\n') :\n",
        "    tokrnized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    for i in range(1,len(tokrnized_sentence)):\n",
        "        input_sq.append(tokrnized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "dBs-_v9sw2z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not have same size ub data\n",
        "# 0 padding in data\n",
        "k1=max([len(x) for x in input_sq])"
      ],
      "metadata": {
        "id": "vuIQ7M1ew7Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_iput=pad_sequences(input_sq,maxlen=k1,padding='pre')"
      ],
      "metadata": {
        "id": "r2S6KXQXw9ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=padded_iput[:,:-1]"
      ],
      "metadata": {
        "id": "SUtp-eSGw__1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=padded_iput[:,-1]"
      ],
      "metadata": {
        "id": "YT1tPK1yxC5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y=to_categorical(y,num_classes=408,)"
      ],
      "metadata": {
        "id": "iHvVziKzxE2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense"
      ],
      "metadata": {
        "id": "1dD5j31pxHHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(408, 100, input_length=56))\n",
        "model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(408, activation='softmax'))"
      ],
      "metadata": {
        "id": "AYcDgc4txJ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5G51D1TpxL1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ZibxTpLXxOHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=70)"
      ],
      "metadata": {
        "id": "Av5Aa-9txQnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k1"
      ],
      "metadata": {
        "id": "zqCBf2K85hlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict_next_words(model, tokenizer, text, n=5):\n",
        "    tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "    for i in range(n):\n",
        "        padded = pad_sequences([tokens], maxlen=109, padding='pre')\n",
        "        pred = model.predict(padded, verbose=0)\n",
        "        next_token = np.argmax(pred)\n",
        "        tokens.append(next_token)\n",
        "\n",
        "    # Get last n words\n",
        "    result = []\n",
        "    for token in tokens[-n:]:\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == token:\n",
        "                result.append(word)\n",
        "                break\n",
        "\n",
        "    return result\n",
        "\n",
        "# Usage\n",
        "text = \"variance\"\n",
        "next_words = predict_next_words(model, tokenizer, text, 5)\n",
        "print(f\"{text} {' '.join(next_words)}\")"
      ],
      "metadata": {
        "id": "p3P2jKUYxSt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"real-life\"\n",
        "for i in range(14):\n",
        "  tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "  padded = pad_sequences([tokens], maxlen=100, padding='pre')\n",
        "  pso = np.argmax(model.predict(padded, verbose=0))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == pso:\n",
        "          text = text + \" \" + word\n",
        "          print(text)"
      ],
      "metadata": {
        "id": "yiZsw41149lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-o_uoUi580D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}