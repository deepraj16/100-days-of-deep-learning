{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNMTLlCiQ8118vJOaHxT3vE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepraj16/100-days-of-deep-learning/blob/main/day_51Understanding_uncoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjJ5G6isMdHa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "X2u0g75gOQLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n"
      ],
      "metadata": {
        "id": "ghChi8_UNnGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"umasrikakollu72/hindi-english-truncated-corpus\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "EyKunF0tNvpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PuSZNNLTODJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/kaggle/input/hindi-english-truncated-corpus\"\n",
        "print(os.listdir(path))"
      ],
      "metadata": {
        "id": "71y7xxZtOGAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(path, \"Hindi_English_Truncated_Corpus.csv\"))"
      ],
      "metadata": {
        "id": "O6u-Kc43PFGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "nE0iSOeoPL1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['english_sentence'][0]"
      ],
      "metadata": {
        "id": "mNKsb0OkbeaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hindi_sentence'][0]"
      ],
      "metadata": {
        "id": "zpLitp2zbny8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:30000,:]"
      ],
      "metadata": {
        "id": "XVvWmLUnbqjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = df\n",
        "lines = lines[lines['source'] == 'ted'][['english_sentence', 'hindi_sentence']].dropna().drop_duplicates()\n",
        "lines = lines.sample(n=25000, random_state=42)\n",
        "print(f\"Filtered dataset shape: {lines.shape}\")"
      ],
      "metadata": {
        "id": "1xWqxxn0POAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines['english_sentence'][0]"
      ],
      "metadata": {
        "id": "kbUJByUocjn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def clean_text(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    text = text.translate(str.maketrans('', '', string.digits))\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "id": "Rh8xexryPVam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines['english_sentence'] = lines['english_sentence'].apply(clean_text)\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(clean_text)\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'start_ ' + x + ' _end')"
      ],
      "metadata": {
        "id": "H6vgCGotPX6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(lines['english_sentence'])\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(lines['english_sentence'])\n",
        "\n",
        "hin_tokenizer = Tokenizer(filters='')\n",
        "hin_tokenizer.fit_on_texts(lines['hindi_sentence'])\n",
        "hin_seq = hin_tokenizer.texts_to_sequences(lines['hindi_sentence'])"
      ],
      "metadata": {
        "id": "U61kh5IWPkD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English vocabulary size: {len(eng_tokenizer.word_index)}\")\n",
        "print(f\"Hindi vocabulary size: {len(hin_tokenizer.word_index)}\")"
      ],
      "metadata": {
        "id": "5XP-F8xvh0CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len = max(len(seq) for seq in eng_seq)\n",
        "max_hin_len = max(len(seq) for seq in hin_seq)\n",
        "\n",
        "encoder_input = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
        "decoder_input = pad_sequences(hin_seq, maxlen=max_hin_len, padding='post')\n",
        "print(f\"Encoder input shape: {encoder_input.shape}\")\n",
        "print(f\"Decoder input shape: {decoder_input.shape}\")"
      ],
      "metadata": {
        "id": "AZd5-9RbPzAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target = np.zeros((decoder_input.shape[0], decoder_input.shape[1], 1))\n",
        "decoder_target[:, 0:-1, 0] = decoder_input[:, 1:]\n",
        "print(f\"Decoder target shape: {decoder_target.shape}\")"
      ],
      "metadata": {
        "id": "ONrdUL8UP1XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Split data into train and validation sets\n",
        "train_encoder_input, val_encoder_input, train_decoder_input, val_decoder_input, train_decoder_target, val_decoder_target = train_test_split(\n",
        "    encoder_input, decoder_input, decoder_target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_encoder_input)}\")\n",
        "print(f\"Validation samples: {len(val_encoder_input)}\")"
      ],
      "metadata": {
        "id": "BbIEij91QAkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
        "embedding_dim = 256\n",
        "hidden_units = 512"
      ],
      "metadata": {
        "id": "L98u1RA3QDSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English vocab size: {eng_vocab_size}\")\n",
        "print(f\"Hindi vocab size: {hin_vocab_size}\")"
      ],
      "metadata": {
        "id": "9csFSKhsQEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Build the encoder-decoder model\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_eng_len,))\n",
        "encoder_embedding = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True, use_cudnn=False)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_hin_len,))\n",
        "decoder_embedding = Embedding(hin_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, use_cudnn=False)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(hin_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "YYAaoWcsQUTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Cell 17: Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")\n",
        "print(\"Model summary:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0_rwrCHiQdp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nInput shapes:\")\n",
        "print(f\"Encoder input: {train_encoder_input.shape}\")\n",
        "print(f\"Decoder input: {train_decoder_input.shape}\")\n",
        "print(f\"Decoder target: {train_decoder_target.shape}\")\n"
      ],
      "metadata": {
        "id": "XBv8XwUhjfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "# Start with a smaller batch size and fewer epochs for testing\n",
        "history = model.fit(\n",
        "    [train_encoder_input, train_decoder_input],\n",
        "    train_decoder_target,\n",
        "    batch_size=32,  # Reduced batch size\n",
        "    epochs=5,       # Reduced epochs for initial testing\n",
        "    validation_data=([val_encoder_input, val_decoder_input], val_decoder_target),\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "kAemEg__Q4oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for 5 more epochs\n",
        "print(\"Continuing training for 5 more epochs...\")\n",
        "history_continued = model.fit(\n",
        "    [train_encoder_input, train_decoder_input],\n",
        "    train_decoder_target,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=([val_encoder_input, val_decoder_input], val_decoder_target),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "3n7OmK7fRFEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for 5 more epochs\n",
        "print(\"Continuing training for 5 more epochs...\")\n",
        "history_continued = model.fit(\n",
        "    [train_encoder_input, train_decoder_input],\n",
        "    train_decoder_target,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=([val_encoder_input, val_decoder_input], val_decoder_target),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "6l62_n3sRJaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for 5 more epochs\n",
        "print(\"Continuing training for 5 more epochs...\")\n",
        "history_continued = model.fit(\n",
        "    [train_encoder_input, train_decoder_input],\n",
        "    train_decoder_target,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_data=([val_encoder_input, val_decoder_input], val_decoder_target),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "f4oiJ2eji682"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Build inference models\n",
        "# Encoder model for inference\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder model for inference\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Use the same decoder layers but for inference\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_embedding_inf = Embedding(hin_vocab_size, embedding_dim)(decoder_inputs_single)\n",
        "decoder_lstm_inf = LSTM(hidden_units, return_sequences=True, return_state=True, use_cudnn=False)\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm_inf(\n",
        "    decoder_embedding_inf, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states_inf = [state_h_inf, state_c_inf]\n",
        "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs_inf] + decoder_states_inf\n",
        ")\n",
        "\n",
        "print(\"Inference models created successfully!\")\n"
      ],
      "metadata": {
        "id": "EU-rNi2FmpDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 21: Define translation function\n",
        "def translate_sentence(input_sentence):\n",
        "    # Clean and tokenize input sentence\n",
        "    input_sentence = clean_text(input_sentence)\n",
        "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
        "\n",
        "    # Encode the input\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Set the first character of target sequence with the start character\n",
        "    target_seq[0, 0] = hin_tokenizer.word_index.get('start_', 1)\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word, index in hin_tokenizer.word_index.items():\n",
        "            if sampled_token_index == index:\n",
        "                if word != 'start_' and word != '_end':\n",
        "                    decoded_sentence += ' ' + word\n",
        "                sampled_word = word\n",
        "                break\n",
        "\n",
        "        if sampled_word == '_end' or len(decoded_sentence.split()) > max_hin_len:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "KOQh7FuKn1ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 22: Test the translation function\n",
        "print(\"Testing translation function:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Test with some examples from validation set\n",
        "test_sentences = [\n",
        "    \"hello how are you\",\n",
        "    \"what is your name\",\n",
        "    \"i am fine thank you\",\n",
        "    \"where are you from\",\n",
        "    \"good morning\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence(sentence)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Hindi: {translation}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "GKNBB053n5h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 23: Evaluate on validation set\n",
        "print(\"Evaluating model on validation set...\")\n",
        "val_loss, val_accuracy = model.evaluate(\n",
        "    [val_encoder_input, val_decoder_input],\n",
        "    val_decoder_target,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Cell 24: Save the model and tokenizers\n",
        "print(\"Saving model and tokenizers...\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('translation_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "wE1vgOopn-ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizers\n",
        "import pickle\n",
        "with open('eng_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(eng_tokenizer, f)\n",
        "\n",
        "with open('hin_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(hin_tokenizer, f)\n",
        "\n",
        "print(\"Model and tokenizers saved successfully!\")\n",
        "\n",
        "# Cell 25: Interactive translation function\n",
        "def interactive_translation():\n",
        "    print(\"Interactive Translation System\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    while True:\n",
        "        english_sentence = input(\"Enter English sentence: \")\n",
        "        if english_sentence.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            hindi_translation = translate_sentence(english_sentence)\n",
        "            print(f\"Hindi translation: {hindi_translation}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in translation: {e}\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Uncomment the line below to start interactive translation\n",
        "# interactive_translation()"
      ],
      "metadata": {
        "id": "r8pOs45loDt1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}